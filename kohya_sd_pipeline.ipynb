{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import time\n",
    "from PIL import Image, ImageTk\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from PIL import Image, ImageTk\n",
    "from IPython.display import display\n",
    "import datetime\n",
    "import math\n",
    "from decimal import Decimal\n",
    "\n",
    "\n",
    "# 获取触发词\n",
    "def get_trigger(train_instance_images_dir):\n",
    "    for root, dirs, files in os.walk(train_instance_images_dir):\n",
    "        if len(dirs)!=0:\n",
    "            for dir_ in dirs:\n",
    "                pattern = r\"^\\d\"\n",
    "                match = re.search(pattern, dir_)\n",
    "                if match is not None:\n",
    "                    if dir_.find(\" \") != -1:\n",
    "                        trigger=dir_.split(\" \")[0]\n",
    "                        trigger=dir_.split(\"_\")[1]\n",
    "                    else:\n",
    "                        trigger=dir_.split(\"_\")[1]\n",
    "                    return trigger\n",
    "    return \"\"\n",
    "\n",
    "# 获取图片张数\n",
    "def get_train_image_num(train_instance_images_dir):\n",
    "    trigger=get_trigger(train_instance_images_dir)\n",
    "    for root, dirs, files in os.walk(train_instance_images_dir):\n",
    "        if len(dirs)!=0:\n",
    "            for dir_ in dirs:\n",
    "                if dir_.find(trigger) != -1:\n",
    "                    image_num = len(\n",
    "                        [\n",
    "                            f\n",
    "                            for f, lower_f in (\n",
    "                                (file, file.lower()) for file in os.listdir(f'{train_instance_images_dir}/{dir_}')\n",
    "                            )\n",
    "                            if lower_f.endswith(('.jpg', '.jpeg', '.png', '.webp'))\n",
    "                        ]\n",
    "                    )\n",
    "                    return image_num\n",
    "    return 0\n",
    "\n",
    "# 获取重复次数\n",
    "def get_repeats_nums(train_instance_images_dir):\n",
    "    trigger=get_trigger(train_instance_images_dir)\n",
    "    for root, dirs, files in os.walk(train_instance_images_dir):\n",
    "        if len(dirs) != 0:\n",
    "            for dir_ in dirs:\n",
    "                if dir_.find(trigger) != -1:\n",
    "                    pattern = r'\\d+'\n",
    "                    match = re.findall(pattern, dir_)\n",
    "                    return int(match[0])\n",
    "    return 0\n",
    "\n",
    "# 获取对应路径下的所有目录，返回list\n",
    "def get_dirs(path):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        if len(dirs) != 0:\n",
    "            return dirs\n",
    "    return {}\n",
    "\n",
    "# 获取预热的总步数\n",
    "def get_warmup_steps(lr_warmup, train_batch_size, train_instance_images_dir):\n",
    "    image_num = get_train_image_num(train_instance_images_dir)\n",
    "    repeats_num = get_repeats_nums(train_instance_images_dir)\n",
    "    print(f'image_num = {image_num}')\n",
    "    print(f'repeats_num = {repeats_num}')\n",
    "\n",
    "    if lr_warmup > 0:\n",
    "        repeats = int(image_num) * int(repeats_num)\n",
    "        max_train_steps = int(\n",
    "            math.ceil(float(repeats) / int(train_batch_size) * int(epoch))\n",
    "        )\n",
    "        lr_warmup_steps = round(float(int(lr_warmup) * int(max_train_steps) / 100))\n",
    "        return lr_warmup_steps\n",
    "    return 0\n",
    "\n",
    "\n",
    "# 切换模型\n",
    "def cut_config(sd_conf_path, args):\n",
    "    if args[\"config_data\"] is None:\n",
    "        return\n",
    "    conf = None\n",
    "    with open(sd_conf_path, \"r\") as f:\n",
    "        try:\n",
    "            conf=json.load(f)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON Decode Error:{e.msg},lineno:{e.lineno},colno:{e.colno}\")\n",
    "            return\n",
    "\n",
    "    for field in args[\"config_data\"]:\n",
    "        if field in conf.keys():\n",
    "            conf[field]=args[\"config_data\"][field]\n",
    "    with open(sd_conf_path, \"w\") as f:\n",
    "        json.dump(conf, f, indent=4)\n",
    "    f.close()\n",
    "    return\n",
    "\n",
    "# 初始化预览结果文件\n",
    "def writ_html(log_html, temp, style):\n",
    "    with open(log_html, \"w+\") as f:\n",
    "        content=f.read()\n",
    "        if content is None or content == \"\":\n",
    "            with open(temp, \"r\") as tf:\n",
    "                t_content=tf.read()\n",
    "                t_content=t_content.replace(\"%title%\", style)\n",
    "                t_content=t_content.replace(\"%style%\", style)\n",
    "                f.write(t_content)\n",
    "                print(\"init html log file success!\")\n",
    "        f.close()\n",
    "    return\n",
    "\n",
    "\n",
    "# 推理代码初始化\n",
    "def write_train_codes(sd_path):\n",
    "    new_code = '''\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import importlib\n",
    "import signal\n",
    "import re\n",
    "from typing import Dict, List, Any\n",
    "from packaging import version\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"xformers\").addFilter(lambda record: 'A matching Triton is not available' not in record.getMessage())\n",
    "\n",
    "from modules import errors\n",
    "from modules.call_queue import wrap_queued_call, queue_lock, wrap_gradio_gpu_call\n",
    "\n",
    "import torch\n",
    "\n",
    "if \".dev\" in torch.__version__ or \"+git\" in torch.__version__:\n",
    "    torch.__long_version__ = torch.__version__\n",
    "    torch.__version__ = re.search(r'[\\d.]+[\\d]', torch.__version__).group(0)\n",
    "\n",
    "from modules import shared, devices, ui_tempdir\n",
    "shared.parser.add_argument(\"--output_dir\", type=str, default=None, help=\"output_dir\")\n",
    "import modules.codeformer_model as codeformer\n",
    "import modules.face_restoration\n",
    "import modules.gfpgan_model as gfpgan\n",
    "import modules.img2img\n",
    "\n",
    "import modules.lowvram\n",
    "import modules.paths\n",
    "import modules.scripts\n",
    "import modules.sd_hijack\n",
    "import modules.sd_models\n",
    "import modules.sd_vae\n",
    "import modules.txt2img\n",
    "import modules.script_callbacks\n",
    "import modules.textual_inversion.textual_inversion\n",
    "import modules.progress\n",
    "\n",
    "import modules.ui\n",
    "from modules import modelloader\n",
    "from modules.shared import cmd_opts, opts\n",
    "import modules.hypernetworks.hypernetwork\n",
    "\n",
    "from modules.processing import StableDiffusionProcessingTxt2Img, StableDiffusionProcessingImg2Img, process_images\n",
    "import base64\n",
    "import io\n",
    "from fastapi import HTTPException\n",
    "from io import BytesIO\n",
    "import piexif\n",
    "import piexif.helper\n",
    "from PIL import PngImagePlugin, Image\n",
    "\n",
    "from modules import import_hook, errors, extra_networks, ui_extra_networks_checkpoints\n",
    "from modules import extra_networks_hypernet, ui_extra_networks_hypernets, ui_extra_networks_textual_inversion\n",
    "from modules import shared, devices, sd_samplers, upscaler, extensions, localization, ui_tempdir, ui_extra_networks\n",
    "\n",
    "\n",
    "def check_versions():\n",
    "    if shared.cmd_opts.skip_version_check:\n",
    "        return\n",
    "\n",
    "\n",
    "def initialize():\n",
    "    check_versions()\n",
    "    extensions.list_extensions()\n",
    "    localization.list_localizations(cmd_opts.localizations_dir)\n",
    "\n",
    "    if cmd_opts.ui_debug_mode:\n",
    "        shared.sd_upscalers = upscaler.UpscalerLanczos().scalers\n",
    "        modules.scripts.load_scripts()\n",
    "        return\n",
    "\n",
    "    modelloader.cleanup_models()\n",
    "    modules.sd_models.setup_model()\n",
    "    codeformer.setup_model(cmd_opts.codeformer_models_path)\n",
    "    gfpgan.setup_model(cmd_opts.gfpgan_models_path)\n",
    "    modelloader.list_builtin_upscalers()\n",
    "    modules.scripts.load_scripts()\n",
    "    modelloader.load_upscalers()\n",
    "\n",
    "    modules.sd_vae.refresh_vae_list()\n",
    "    modules.textual_inversion.textual_inversion.list_textual_inversion_templates()\n",
    "\n",
    "    try:\n",
    "        modules.sd_models.load_model()\n",
    "    except Exception as e:\n",
    "        errors.display(e, \"loading stable diffusion model\")\n",
    "        print(\"\", file=sys.stderr)\n",
    "        print(\"Stable diffusion model failed to load, exiting\", file=sys.stderr)\n",
    "        exit(1)\n",
    "\n",
    "    shared.opts.data[\"sd_model_checkpoint\"] = shared.sd_model.sd_checkpoint_info.title\n",
    "\n",
    "    shared.opts.onchange(\"sd_model_checkpoint\", wrap_queued_call(lambda: modules.sd_models.reload_model_weights()))\n",
    "    shared.opts.onchange(\"sd_vae\", wrap_queued_call(lambda: modules.sd_vae.reload_vae_weights()), call=False)\n",
    "    shared.opts.onchange(\"sd_vae_as_default\", wrap_queued_call(lambda: modules.sd_vae.reload_vae_weights()), call=False)\n",
    "    shared.opts.onchange(\"temp_dir\", ui_tempdir.on_tmpdir_changed)\n",
    "    modules.script_callbacks.before_ui_callback()\n",
    "\n",
    "    def sigint_handler(sig, frame):\n",
    "        print(f'Interrupted with signal {sig} in {frame}')\n",
    "        os._exit(0)\n",
    "\n",
    "    signal.signal(signal.SIGINT, sigint_handler)\n",
    "\n",
    "\n",
    "class Handler():\n",
    "    def __init__(self, path=\"\"):\n",
    "        initialize()\n",
    "        self.shared = shared\n",
    "\n",
    "    def __call__(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "        print(\"-----inputs:\", data[\"inputs\"])\n",
    "        print(\"-----output_dir:\", data[\"output_dir\"])\n",
    "        args = {\n",
    "            \"do_not_save_samples\": True,\n",
    "            \"do_not_save_grid\": True,\n",
    "            \"outpath_samples\": \"./output\",\n",
    "            \"prompt\": \"lora:koreanDollLikeness_v15:0.66, best quality, ultra high res, (photorealistic:1.4), 1girl, beige sweater, black choker, smile, laughing, bare shoulders, solo focus, ((full body), (brown hair:1), looking at viewer\",\n",
    "            \"negative_prompt\": \"paintings, sketches, (worst quality:2), (low quality:2), (normal quality:2), lowres, normal quality, ((monochrome)), ((grayscale)), skin spots, acnes, skin blemishes, age spot, glans, (ugly:1.331), (duplicate:1.331), (morbid:1.21), (mutilated:1.21), (tranny:1.331), mutated hands, (poorly drawn hands:1.331), blurry, 3hands,4fingers,3arms, bad anatomy, missing fingers, extra digit, fewer digits, cropped, jpeg artifacts,poorly drawn face,mutation,deformed\",\n",
    "            \"sampler_name\": \"DPM++ SDE Karras\",\n",
    "            \"steps\": 20,  # 25\n",
    "            \"cfg_scale\": 8,\n",
    "            \"width\": 512,\n",
    "            \"height\": 768,\n",
    "            \"seed\": -1,\n",
    "            \"n_iter\":1,\n",
    "        }\n",
    "        if data[\"inputs\"]:\n",
    "            for field in args:\n",
    "                if field in data[\"inputs\"].keys():\n",
    "                    args[field] = data[\"inputs\"][field]\n",
    "\n",
    "        print(\"prompt:\", args)\n",
    "        p = StableDiffusionProcessingTxt2Img(sd_model=self.shared.sd_model, **args)\n",
    "        processed = process_images(p)\n",
    "        ret={}\n",
    "        output_dir = data['output_dir']\n",
    "        idx=0\n",
    "        for img in processed.images:\n",
    "            path=save_image(img,output_dir)\n",
    "            ret[str(idx)] = path\n",
    "            idx=idx+1\n",
    "        return {\n",
    "            \"img_path\": ret\n",
    "        }\n",
    "\n",
    "\n",
    "import io\n",
    "from modules.shared import cmd_opts, opts\n",
    "from PIL import PngImagePlugin, Image\n",
    "import piexif\n",
    "import piexif.helper\n",
    "from fastapi import HTTPException\n",
    "import base64\n",
    "\n",
    "def save_image(image, output_dir):\n",
    "    with io.BytesIO() as output_bytes:\n",
    "\n",
    "        if opts.samples_format.lower() == 'png':\n",
    "            use_metadata = False\n",
    "            metadata = PngImagePlugin.PngInfo()\n",
    "            for key, value in image.info.items():\n",
    "                if isinstance(key, str) and isinstance(value, str):\n",
    "                    metadata.add_text(key, value)\n",
    "                    use_metadata = True\n",
    "            image.save(output_bytes, format=\"PNG\", pnginfo=(metadata if use_metadata else None),\n",
    "                       quality=opts.jpeg_quality)\n",
    "\n",
    "        elif opts.samples_format.lower() in (\"jpg\", \"jpeg\", \"webp\"):\n",
    "            parameters = image.info.get('parameters', None)\n",
    "            exif_bytes = piexif.dump({\n",
    "                \"Exif\": {\n",
    "                    piexif.ExifIFD.UserComment: piexif.helper.UserComment.dump(parameters or \"\", encoding=\"unicode\")}\n",
    "            })\n",
    "            if opts.samples_format.lower() in (\"jpg\", \"jpeg\"):\n",
    "                image.save(output_bytes, format=\"JPEG\", exif=exif_bytes, quality=opts.jpeg_quality)\n",
    "            else:\n",
    "                image.save(output_bytes, format=\"WEBP\", exif=exif_bytes, quality=opts.jpeg_quality)\n",
    "\n",
    "        else:\n",
    "            raise HTTPException(status_code=500, detail=\"Invalid image format\")\n",
    "\n",
    "        bytes_data = output_bytes.getvalue()\n",
    "\n",
    "    img_name=time.time()\n",
    "    img_path=f\"{output_dir}/{img_name}.png\"\n",
    "    with open(img_path, 'wb') as f:\n",
    "        f.write(bytes_data)\n",
    "    return img_path\n",
    "\n",
    "\n",
    "    '''\n",
    "\n",
    "#     inference_args_str=str(inference_args)\n",
    "    input_str= '''\n",
    "if __name__ == '__main__':\n",
    "    import json\n",
    "    cmd_opts = shared.parser.parse_args()\n",
    "    options_cli = cmd_opts.options_cli\n",
    "    # 配置文件\n",
    "    input_file = f\"{options_cli}/input.json\"\n",
    "    out_put_json = f\"{options_cli}/output.json\"\n",
    "    # 输出路径\n",
    "    output_dir = f\"{options_cli}/output\"\n",
    "    h_func = Handler()\n",
    "\n",
    "    input_ = None\n",
    "    with open(input_file, 'r') as f:\n",
    "        input_ = json.load(f)\n",
    "    inp = {\n",
    "        'inputs': input_,\n",
    "        'output_dir': output_dir\n",
    "    }\n",
    "    res = h_func(inp)\n",
    "    with open(out_put_json, 'w') as f:\n",
    "        json.dump(res, f)\n",
    "    print(\"txt2imageSuccess:\")\n",
    "    print(res)\n",
    "    '''\n",
    "#     .format(inference_args_str)\n",
    "\n",
    "    new_code = new_code + input_str\n",
    "    with open(sd_path, \"w\") as f:\n",
    "        f.write(new_code)\n",
    "        print(\"step: infernec code write success!\")\n",
    "    return\n",
    "\n",
    "# 两个虚拟环境的桥梁\n",
    "def write_train_bash(bash_file_path, sd_path, output_dir):\n",
    "    new_code = f'''\n",
    "    #!/bin/bash\n",
    "    SD_WORKER_DIR={sd_path}\n",
    "    source \"$SD_WORKER_DIR/venv/bin/activate\"\n",
    "    echo \"python /data/aigc/stable-diffusion-webui/handler.py {output_dir}\"\n",
    "    python /data/aigc/stable-diffusion-webui/handler.py --options-cli={output_dir} --disable-nan-check --disable-safe-unpickle --enable-insecure-extension-access --no-half --xformers --deepdanbooru\n",
    "    '''\n",
    "    with open(bash_file_path, \"w\") as f:\n",
    "        f.write(new_code)\n",
    "        print(\"step: infernec bash code write success!\")\n",
    "\n",
    "# 更新结果预览环境中html代码引用的静态文件版本号\n",
    "def writef(md_file_name, data, log_html):\n",
    "    # *.md\n",
    "    header_str='''| 时间 | 版本 | trigger | 执行时长(s) | lr_scheduler | lr_warmup | epoch | learning_rate | text_encoder_lr | unet_lr | train_batch_size | network_dim | network_alpha | img1 | img2 | img3 | img4 |\n",
    "| :----: | :----: | :----: | :----:| :----: | :----:| :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: |'''\n",
    "    if os.path.exists(md_file_name) == False:\n",
    "        with open(md_file_name, \"a\") as f:\n",
    "            f.write(f\"{header_str}\\n{data}\\n\")\n",
    "            f.close()\n",
    "    else:\n",
    "        with open(md_file_name, \"a\") as f:\n",
    "            f.write(f\"{data}\\n\")\n",
    "            f.close()\n",
    "\n",
    "    with open(log_html, \"r\") as r_hf:\n",
    "        ctt=r_hf.read()\n",
    "        r_hf.close()\n",
    "        with open(log_html, \"w\") as w_hf:\n",
    "            v_str=str(time.time())\n",
    "            ctt=ctt.replace(\"%version%\", v_str)\n",
    "            new_st = re.sub(r'(?<=v=)\\d+\\.\\d+', v_str, ctt)\n",
    "            w_hf.write(new_st)\n",
    "            w_hf.close()\n",
    "\n",
    "\n",
    "\n",
    "##############################\n",
    "#           目录规划          #\n",
    "##############################\n",
    "##############################\n",
    "#        初始化相关文件        #\n",
    "##############################\n",
    "# 1.先规划kohya的训练目录，主要是人脸照片的目录\n",
    "# 2.训练好的Lora 模型名称\n",
    "# 3.规划sd工作目录\n",
    "# 4.规划风格目录，存放风格的prompt参数文件，以及出图后的output目录\n",
    "\n",
    "\n",
    "# 风格模板最终的推理目录，包含风格模板的相关的prompt信息\n",
    "temp_dir='/data/aigc/inferenc_template/template'\n",
    "# 风格模板名\n",
    "style='snow_v1.1'\n",
    "\n",
    "### 文件包含\n",
    "# 1. {style}.md   训练+推理后的日志文件\n",
    "# 2. {style}.html HTML预览文件\n",
    "# 3. {style}.json prompt推理文件\n",
    "# {style}.html 文件通过模板文件 “._template.html“ 生成\n",
    "log_html_template=f\"{temp_dir}/._template.html\"\n",
    "log_html=f\"{temp_dir}/{style}.html\"\n",
    "log_name=f\"{temp_dir}/{style}.md\"\n",
    "\n",
    "\n",
    "writ_html(log_html, log_html_template, style)\n",
    "\n",
    "# kohya 工作目录\n",
    "kohya_ss_dir=\"/data/aigc/kohya_ss\"\n",
    "\n",
    "# sd 工作目录\n",
    "sd_worker_dir=\"/data/aigc/stable-diffusion-webui\"\n",
    "# sd 配置文件\n",
    "sd_conf_path=f\"{sd_worker_dir}/config.json\"\n",
    "# sd 注入文件\n",
    "sd_handler_path=f\"{sd_worker_dir}/handler.py\"\n",
    "# base 模型\n",
    "train_base_model=f\"{sd_worker_dir}/models/Stable-diffusion/v1-5-pruned.ckpt\"\n",
    "\n",
    "# kohya 入口文件\n",
    "train_network=f\"{kohya_ss_dir}/train_network.py\"\n",
    "# kohya->sd 的中间文件\n",
    "bash_file_path=f\"{sd_worker_dir}/sd_inferenc.sh\"\n",
    "\n",
    "# review_url\n",
    "review_url=\"http://34.231.135.235/template\"\n",
    "\n",
    "# Lora模型名称\n",
    "# >>>>>>>规定人物的时候需要调整这里\n",
    "lora_name=\"\"\n",
    "\n",
    "\n",
    "\n",
    "######################\n",
    "# Lora 模型训练参数\n",
    "######################\n",
    "### text_encoder 的学习率，常取定值5e-5，一般将它调成 unet_lr 的 10-15分之一，\n",
    "### text_encoder 调整到 unet_lr 1/8 附近也是常见的做法。调低该参数有助于更多学习文本编码器（对 tag 更敏感）。\n",
    "learning_rate=0.0001\n",
    "\n",
    "#版本文件命名参数之一\n",
    "text_encoder_lr = 5e-5\n",
    "#版本文件命名参数之二\n",
    "unet_lr = 0.0005\n",
    "# constant,constant_with_warmup,cosine,cosine_with_restarts,linear,polynomial,adafactor\n",
    "lr_scheduler=\"cosine\"\n",
    "\n",
    "# 预热比例（百分比：0~100）\n",
    "lr_warmup=0\n",
    "train_batch_size=4\n",
    "epoch=15\n",
    "\n",
    "network_dim=64\n",
    "network_alpha=32\n",
    "\n",
    "\n",
    "######################\n",
    "# Lora 模型推理权重\n",
    "######################\n",
    "lora_weight=\"0.7\"\n",
    "output_image_size = 4\n",
    "style_prompt=f\"{temp_dir}/{style}.json\"\n",
    "\n",
    "print(\"style_prompt:\",style_prompt)\n",
    "# print(f\"train_output:{train_output}\")\n",
    "\n",
    "train_instance_set_dir = f\"{kohya_ss_dir}/train/men\"\n",
    "train_instances = get_dirs(train_instance_set_dir)\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# images = [np.random.rand(100,100) for i in range(4)]\n",
    "# fig = plt.figure(figsize=(10, 10), dpi=100)\n",
    "# for i in range(len(images)):\n",
    "#     ax = fig.add_subplot(1, len(images), i+1)\n",
    "#     ax.imshow(images[i], cmap='gray')\n",
    "#     ax.axis('off')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# train_instances={\"bairennan018\"}\n",
    "for its in train_instances:\n",
    "    lora_name = its\n",
    "    if its.find(\"nv\") != -1:\n",
    "        sex=\"woman\"\n",
    "    else:\n",
    "        sex=\"man\"\n",
    "#     print(\"its:\", its)\n",
    "    # instance 人物照片\n",
    "    train_instance_images=f\"{train_instance_set_dir}/{lora_name}/images/instance\"\n",
    "    train_output=f\"{sd_worker_dir}/models/Lora/pipelined/{lora_name}\"\n",
    "    pathlib.Path(train_output).mkdir(parents=True, exist_ok=True)\n",
    "    #获取预热处理步数\n",
    "    warmup_steps = get_warmup_steps(lr_warmup, train_batch_size, train_instance_images)\n",
    "\n",
    "    ######################\n",
    "    # pipeline......\n",
    "    ######################\n",
    "\n",
    "    #########\n",
    "    # 背景：network_dim=32,network_alpha=16,text_encoder_lr0.000191,unet_lr=0.00251 这个参数训练出来的参数在进行推理的时候报错：\n",
    "    # 错误：modules.devices.NansException: A tensor with all NaNs was produced in Unet. This could be either because there's not enough precision to represent the picture, or because your video card does not support half type. Try setting the \"Upcast cross attention layer to float32\" option in Settings > Stable Diffusion or using the --no-half commandline argument to fix this. Use --disable-nan-check commandline argument to disable this check.\n",
    "    #\n",
    "    #\n",
    "    #########\n",
    "    # ,\"polynomial\",\"adafactor\"\n",
    "\n",
    "    #################################################################:start\n",
    "    start_time=time.time()\n",
    "    now_date=time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "    print(f\"current time:{now_date}\\n\")\n",
    "\n",
    "    ######################\n",
    "    # step.1 训练好后的Lora模型名称\n",
    "    ######################\n",
    "\n",
    "    ###version\n",
    "    lora_version=f\"t{text_encoder_lr}_u{unet_lr}_{lr_scheduler}\"\n",
    "    print(f\"###############################lora_version:{lora_version}\")\n",
    "\n",
    "    if lora_version != \"\" and lora_version != None:\n",
    "        lora_name_version=f\"{lora_name}_{lora_version}\"\n",
    "    else:\n",
    "        lora_name_version=lora_name\n",
    "\n",
    "    # 推理后的输出目录\n",
    "    output_dir=f\"{temp_dir}/{style}/{lora_name_version}\"\n",
    "    if os.path.exists(output_dir):\n",
    "        print(f'###############################skip:{lora_name_version}')\n",
    "        continue\n",
    "\n",
    "    ##图片地址封装：\n",
    "    url_path_prefix=f\"{review_url}/{style}/{lora_name_version}/output\"\n",
    "    output_img_dir=f\"{output_dir}/output\"\n",
    "    pathlib.Path(output_img_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    ##推理图片的输入输出文件\n",
    "    prompt_conf=f\"{output_dir}/input.json\"\n",
    "    out_put_json = f\"{output_dir}/output.json\"\n",
    "\n",
    "\n",
    "    # # # # # # # # # # # # # # # #\n",
    "    # step.2 训练模型\n",
    "    # # # # # # # # # # # # # # # #\n",
    "\n",
    "    train_script=f'''\n",
    "    accelerate launch \\\n",
    "    --num_cpu_threads_per_process=2 \"{train_network}\" \\\n",
    "    --enable_bucket \\\n",
    "    --pretrained_model_name_or_path=\"{train_base_model}\" \\\n",
    "    --train_data_dir=\"{train_instance_images}\" \\\n",
    "    --output_dir=\"{train_output}\" \\\n",
    "    --output_name=\"{lora_name_version}\" \\\n",
    "    --resolution=512,512 \\\n",
    "    --bucket_reso_steps=64 \\\n",
    "    --train_batch_size=\"{train_batch_size}\" \\\n",
    "    --learning_rate=\"{learning_rate}\" \\\n",
    "    --text_encoder_lr={text_encoder_lr} \\\n",
    "    --unet_lr={unet_lr} \\\n",
    "    --lr_scheduler=\"{lr_scheduler}\" \\\n",
    "    --max_train_epochs={epoch} \\\n",
    "    --lr_scheduler_num_cycles={epoch} \\\n",
    "    --network_dim={network_dim} \\\n",
    "    --network_alpha={network_alpha} \\\n",
    "    --save_model_as=safetensors \\\n",
    "    --network_module=networks.lora \\\n",
    "    --bucket_no_upscale \\\n",
    "    --cache_latents \\\n",
    "    --xformers \\\n",
    "    --optimizer_type=\"AdamW8bit\" \\\n",
    "    --noise_offset=0.1 \\\n",
    "    --caption_extension=\".txt\" \\\n",
    "    --keep_tokens=\"1\" \\\n",
    "    --shuffle_caption \\\n",
    "    --mixed_precision=\"fp16\" \\\n",
    "    --save_precision=\"fp16\" \\\n",
    "    --seed=\"1234\" \\\n",
    "    --max_data_loader_n_workers=\"0\" '''\n",
    "    if warmup_steps>0:\n",
    "        train_script+=f' --lr_warmup_steps={warmup_steps} '\n",
    "    print(\"train_script:\", train_script)\n",
    "\n",
    "    # --lr_warmup_steps={} \\\n",
    "    # --text_encoder_lr=9e-05 \\\n",
    "    # --unet_lr=0.0004 \\\n",
    "    # --prior_loss_weight=0.3 \\\n",
    "    # --save_every_n_epochs=1 \\\n",
    "    #--logging_dir=\"/data/aigc/kohya_ss/train/lm_tw_nan020/logs/v10.0\" \\\n",
    "    # --lr_scheduler=\"cosine\" \\\n",
    "\n",
    "    trigger=get_trigger(train_instance_images)\n",
    "    #         for root, dirs, files in os.walk(train_instance_images):\n",
    "    #             if len(dirs) !=0:\n",
    "    #                 for dir_ in dirs:\n",
    "    #                     pattern = r\"^\\d\"\n",
    "    #                     match = re.search(pattern, dir_)\n",
    "    #                     if match is not None:\n",
    "    #                         trigger=get_trigger(dir_)\n",
    "\n",
    "    if trigger!=\"\":\n",
    "#         print(\"trigger:\", train_instance_images)\n",
    "        trigger_images={}\n",
    "        for root, dirs, files in os.walk(train_instance_images):\n",
    "            if len(dirs) != 0:\n",
    "                for dir_ in dirs:\n",
    "                    tmp_dir=f\"{train_instance_images}/{dir_}\"\n",
    "                    for r, d, fs in os.walk(tmp_dir):\n",
    "                        for f in fs:\n",
    "                            pth=os.path.join(r, f)\n",
    "                            print(\"pth:\", pth)\n",
    "                            if pth.find(\".png\") != -1 or pth.find(\".jpg\") != -1 or pth.find(\".jpeg\") != -1:\n",
    "                                ig=Image.open(pth)\n",
    "                                display(ig)\n",
    "        ! $train_script\n",
    "    else:\n",
    "        print(\"error: trigger is empty !\")\n",
    "\n",
    "    now_date=time.strftime(\"%m-%d %H:%M:%S\", time.localtime())\n",
    "    print(f\"current time:{now_date}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    # # # # # # # # # # # # # # # #\n",
    "    # step.3 训练模型\n",
    "    # # # # # # # # # # # # # # # #\n",
    "    print(\"step:\\n-------------------------------\\n\\nadjust model inferenc params……\")\n",
    "    # style_template=\"/data/aigc/inferenc_template/template\"\n",
    "    # temp_dir\n",
    "\n",
    "\n",
    "    # sd 切换风格地址\n",
    "    # /data/aigc/stable-diffusion-webui\n",
    "\n",
    "    with open(style_prompt, \"r\") as f:\n",
    "        style_data=json.load(f)\n",
    "        style_data=style_data[sex]\n",
    "        model_name=style_data['_model_name_']\n",
    "        style_data['prompt']=style_data['prompt'] + f\",<lora:{lora_name_version}:{lora_weight}>\"\n",
    "        style_data['prompt']=style_data['prompt'].replace(\"%s\", trigger)\n",
    "        style_data['n_iter']=output_image_size\n",
    "\n",
    "    print(\"styledata:\", style_data)\n",
    "\n",
    "\n",
    "    if model_name==\"\" or model_name is None:\n",
    "        print(\"error: adjust model error, model_name empty\")\n",
    "    else:\n",
    "        params = {\n",
    "            \"config_data\": {\n",
    "                #切换模型名称\n",
    "                \"sd_model_checkpoint\": model_name,\n",
    "                \"sd_checkpoint_hash\":'',\n",
    "                \"sd_vae\":''\n",
    "            }\n",
    "        }\n",
    "\n",
    "    cut_config(sd_conf_path, params)\n",
    "\n",
    "    print(\"adjust params success!\")\n",
    "\n",
    "    now_date=time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "    print(f\"\\ncurrent time:{now_date}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    # # # # # # # # # # # # # # # #\n",
    "    # step.4 推理\n",
    "    # # # # # # # # # # # # # # # #\n",
    "\n",
    "    with open(prompt_conf, 'w') as f:\n",
    "        json.dump(style_data, f, indent=4)\n",
    "\n",
    "    write_train_codes(sd_handler_path)\n",
    "    write_train_bash(bash_file_path, sd_worker_dir, output_dir)\n",
    "\n",
    "    ! chmod +x $bash_file_path\n",
    "\n",
    "    print(\"execute infernec:.....\\n\\n\")\n",
    "    ! $bash_file_path\n",
    "\n",
    "\n",
    "    for root, dirs, files in os.walk(output_img_dir):\n",
    "        for file in files:\n",
    "            path=os.path.join(root, file)\n",
    "            if path.find(\".png\") != -1:\n",
    "                ig=Image.open(path)\n",
    "                display(ig)\n",
    "\n",
    "    #         now_date=time.strftime(\"%m-%d %H:%M:%S\", time.localtime())\n",
    "    #         print(f\"current time:{now_date}\\n\")\n",
    "    now_utc_offset = datetime.datetime.utcnow() + datetime.timedelta(hours=8)\n",
    "    time_str = now_utc_offset.strftime('%m-%d %H:%M:%S')\n",
    "\n",
    "    end_time=time.time()\n",
    "    excution_time=end_time-start_time\n",
    "\n",
    "    print(f\"excution_time: {excution_time}\")\n",
    "\n",
    "    excution_time_int=int(excution_time)\n",
    "    markdown_str=f\"| {time_str} | {lora_version} | {trigger} | {excution_time_int} | {lr_scheduler} | {lr_warmup} | {epoch} | {learning_rate} | {text_encoder_lr} | {unet_lr} | {train_batch_size} | {network_dim} | {network_alpha} | \"\n",
    "    imgs_str=\"\"\n",
    "    idx=1\n",
    "    for root, dirs, files in os.walk(output_img_dir):\n",
    "        for file in files:\n",
    "            path=os.path.join(root, file)\n",
    "            if path.find(\".png\") != -1:\n",
    "                if idx<=4:\n",
    "                    output_images = url_path_prefix + path.replace(output_img_dir, \"\")\n",
    "                    imgs_str=imgs_str+f\" ![图片描述]({output_images}) |\"\n",
    "                idx=idx+1\n",
    "\n",
    "    print(\"imgs_str:\", imgs_str)\n",
    "    markdown_str=markdown_str+imgs_str\n",
    "    print(markdown_str)\n",
    "\n",
    "    writef(log_name, markdown_str, log_html)\n",
    "    #################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
